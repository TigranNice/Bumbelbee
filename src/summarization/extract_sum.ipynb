{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном ноутбуке рассмотрим экстрактивный подход к суммаризации текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "news = pd.read_csv('../data/news10000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new1 = news.iloc[0]\n",
    "text = new1['text']\n",
    "sentences = text.split('.')\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_for_word2vec = news['text'].apply(lambda x: x.split('.')) # split text into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'inspect' has no attribute 'getargspec'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpymorphy2\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m morph \u001b[38;5;241m=\u001b[39m \u001b[43mpymorphy2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMorphAnalyzer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m lemmatized_sentences \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n",
      "File \u001b[1;32me:\\PythonProjects\\Summarization\\.venv\\Lib\\site-packages\\pymorphy2\\analyzer.py:224\u001b[0m, in \u001b[0;36mMorphAnalyzer.__init__\u001b[1;34m(self, path, lang, result_type, units, probability_estimator_cls, char_substitutes)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result_type_orig \u001b[38;5;241m=\u001b[39m result_type\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_char_substitutes(char_substitutes)\n\u001b[1;32m--> 224\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_units\u001b[49m\u001b[43m(\u001b[49m\u001b[43munits\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\PythonProjects\\Summarization\\.venv\\Lib\\site-packages\\pymorphy2\\analyzer.py:235\u001b[0m, in \u001b[0;36mMorphAnalyzer._init_units\u001b[1;34m(self, units_unbound)\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m unit \u001b[38;5;129;01min\u001b[39;00m item[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m--> 235\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_units\u001b[38;5;241m.\u001b[39mappend((\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_unit\u001b[49m\u001b[43m(\u001b[49m\u001b[43munit\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_units\u001b[38;5;241m.\u001b[39mappend((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_unit(item[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]), \u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32me:\\PythonProjects\\Summarization\\.venv\\Lib\\site-packages\\pymorphy2\\analyzer.py:246\u001b[0m, in \u001b[0;36mMorphAnalyzer._bound_unit\u001b[1;34m(self, unit)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_bound_unit\u001b[39m(\u001b[38;5;28mself\u001b[39m, unit):\n\u001b[1;32m--> 246\u001b[0m     unit \u001b[38;5;241m=\u001b[39m \u001b[43munit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    247\u001b[0m     unit\u001b[38;5;241m.\u001b[39minit(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m unit\n",
      "File \u001b[1;32me:\\PythonProjects\\Summarization\\.venv\\Lib\\site-packages\\pymorphy2\\units\\base.py:35\u001b[0m, in \u001b[0;36mBaseAnalyzerUnit.clone\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclone\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32me:\\PythonProjects\\Summarization\\.venv\\Lib\\site-packages\\pymorphy2\\units\\base.py:76\u001b[0m, in \u001b[0;36mBaseAnalyzerUnit._get_params\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     74\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Return a dict with the parameters for this analyzer unit. \"\"\"\u001b[39;00m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m---> 76\u001b[0m         (key, \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, \u001b[38;5;28;01mNone\u001b[39;00m)) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_param_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m     )\n",
      "File \u001b[1;32me:\\PythonProjects\\Summarization\\.venv\\Lib\\site-packages\\pymorphy2\\units\\base.py:70\u001b[0m, in \u001b[0;36mBaseAnalyzerUnit._get_param_names\u001b[1;34m(cls)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m:\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[1;32m---> 70\u001b[0m args, varargs, kw, default \u001b[38;5;241m=\u001b[39m \u001b[43minspect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetargspec\u001b[49m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m)\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(args[\u001b[38;5;241m1\u001b[39m:])\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'inspect' has no attribute 'getargspec'"
     ]
    }
   ],
   "source": [
    "import pymorphy2\n",
    "\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "lemmatized_sentences = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    words = sentence.split()\n",
    "    lemmatized_words = [morph.parse(word)[0].normal_form for word in words]\n",
    "    lemmatized_sentence = ' '.join(lemmatized_words)\n",
    "    lemmatized_sentences.append(lemmatized_sentence)\n",
    "\n",
    "print(lemmatized_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_sentences_word2vec = []\n",
    "\n",
    "for i in sentences_for_word2vec:\n",
    "    for sentence in i:\n",
    "        words = sentence.split()\n",
    "        lemmatized_words = [morph.parse(word)[0].normal_form for word in words]\n",
    "        lemmatized_sentence = ' '.join(lemmatized_words)\n",
    "        lemmatized_sentences_word2vec.append(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_sentences_word2vec = [sentence.split() for sentence in lemmatized_sentences_word2vec]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Экстрактивная суммаризация на основе вхождения общих слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(sentences):\n",
    "    similarities = []\n",
    "    \n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(i+1, len(sentences)):\n",
    "            sentence1 = sentences[i]\n",
    "            sentence2 = sentences[j]\n",
    "            \n",
    "            words1 = sentence1.split()\n",
    "            words2 = sentence2.split()\n",
    "            \n",
    "            common_words = set(words1) & set(words2)\n",
    "            similarity = len(common_words) / (len(words1) + len(words2))\n",
    "            if similarity > 0:\n",
    "                similarities.append((sentence1, sentence2, similarity, i , j))\n",
    "    \n",
    "    return similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranging(similarities, ln):\n",
    "    result = [(0, i) for i in range(ln)]\n",
    "    for i in similarities:\n",
    "        if i[2] > 0:\n",
    "            result[i[3]][0] += 1\n",
    "            result[i[4]][0] += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('неприятель, приблизиться с север к осовца начать артиллерийский борьба с '\n",
      "  'крепость',\n",
      "  'попытка германский пехота пробиться близкий к крепость отразить',\n",
      "  0.10526315789473684,\n",
      "  1,\n",
      "  4),\n",
      " ('в артиллерийский бой принимать участие тяжёлый калибр',\n",
      "  'в галиция мы занять дембица',\n",
      "  0.08333333333333333,\n",
      "  2,\n",
      "  5)]\n",
      " Неприятель, приблизившись с севера к Осовцу начал артиллерийскую борьбу с крепостью\n",
      " Попытка германской пехоты пробиться ближе к крепости отражена\n",
      " В артиллерийском бою принимают участие тяжелые калибры\n",
      " В Галиции мы заняли Дембицу\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "\n",
    "ln_sent = len(lemmatized_sentences)\n",
    "similarities = calculate_similarity(lemmatized_sentences)\n",
    "rang = ranging(similarities, ln_sent)\n",
    "rang.sort(key=lambda x: x[0], reverse=True)\n",
    "pprint(range[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Экстрактивная суммаризация на основе обученных векторных представлений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13229930, 15177920)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(lemmatized_sentences_word2vec, min_count=1)\n",
    "\n",
    "model.train(lemmatized_sentences_word2vec, total_examples=model.corpus_count, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embedding = []\n",
    "\n",
    "for sentence in lemmatized_sentences:\n",
    "    words = sentence.split()\n",
    "    embedding = sum([model.wv[word] for word in words])\n",
    "    sentence_embedding.append(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.31649372, 0, 1),\n",
       " (0.4989151, 0, 2),\n",
       " (0.37511656, 0, 3),\n",
       " (0.37333974, 0, 4),\n",
       " (0.3338399, 0, 5),\n",
       " (0.46066606, 0, 6),\n",
       " (0.37298033, 0, 7),\n",
       " (0.42174768, 0, 8),\n",
       " (0.55801576, 0, 9),\n",
       " (0.14970517, 0, 10),\n",
       " (0.37316564, 1, 2),\n",
       " (0.59505117, 1, 3),\n",
       " (0.6071144, 1, 4),\n",
       " (0.33905736, 1, 5),\n",
       " (0.72942156, 1, 6),\n",
       " (0.2393505, 1, 7),\n",
       " (0.44585058, 1, 8),\n",
       " (0.48829982, 1, 9),\n",
       " (0.20476171, 1, 10),\n",
       " (0.40791482, 2, 3),\n",
       " (0.2869617, 2, 4),\n",
       " (0.26325667, 2, 5),\n",
       " (0.48787487, 2, 6),\n",
       " (0.40069795, 2, 7),\n",
       " (0.4622263, 2, 8),\n",
       " (0.45909458, 2, 9),\n",
       " (0.21284302, 2, 10),\n",
       " (0.37572986, 3, 4),\n",
       " (0.28212357, 3, 5),\n",
       " (0.6137512, 3, 6),\n",
       " (0.20154104, 3, 7),\n",
       " (0.39688435, 3, 8),\n",
       " (0.4170017, 3, 9),\n",
       " (0.60179204, 3, 10),\n",
       " (0.43817982, 4, 5),\n",
       " (0.55983496, 4, 6),\n",
       " (0.26581037, 4, 7),\n",
       " (0.38130057, 4, 8),\n",
       " (0.47588485, 4, 9),\n",
       " (0.18579806, 4, 10),\n",
       " (0.40776572, 5, 6),\n",
       " (0.38305175, 5, 7),\n",
       " (0.41879508, 5, 8),\n",
       " (0.7073965, 5, 9),\n",
       " (0.095969535, 5, 10),\n",
       " (0.38034979, 6, 7),\n",
       " (0.6048609, 6, 8),\n",
       " (0.61098987, 6, 9),\n",
       " (0.21413279, 6, 10),\n",
       " (0.29427147, 7, 8),\n",
       " (0.51393384, 7, 9),\n",
       " (-0.03736831, 7, 10),\n",
       " (0.669873, 8, 9),\n",
       " (0.1432222, 8, 10),\n",
       " (0.10793455, 9, 10)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity_matrix = []\n",
    "\n",
    "for i in range(len(sentence_embedding)):\n",
    "    for j in range(i+1, len(sentence_embedding)):\n",
    "        embedding1 = sentence_embedding[i]\n",
    "        embedding2 = sentence_embedding[j]\n",
    "        if isinstance(embedding1, int) or isinstance(embedding2, int):\n",
    "            continue\n",
    "\n",
    "        if embedding1.all() == 0 or embedding2.all() == 0:\n",
    "            continue\n",
    "\n",
    "        similarity = cosine_similarity([embedding1], [embedding2])[0][0]\n",
    "        similarity_matrix.append((similarity, i, j))\n",
    "\n",
    "similarity_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_variables = sorted(similarity_matrix, key=lambda x: x[0], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Неприятель, приблизившись с севера к Осовцу начал артиллерийскую борьбу с крепостью\n",
      " Большая колонна, отступавшая по шоссе от Перемышля к Саноку, обстреливалась с высот нашей батареей и бежала, бросив парки, обоз и автомобили\n",
      " В Галиции мы заняли Дембицу\n",
      " На перевале Ужок мы разбили неприятельский отряд, взяли его артиллерию и много пленных и, продолжая преследовать, вступили в пределы Венгрии\n"
     ]
    }
   ],
   "source": [
    "result = ranked_variables[:2]\n",
    "\n",
    "for i in range(2):\n",
    "    print(sentences[result[i][1]])\n",
    "    print(sentences[result[i][2]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
